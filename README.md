# Web Scrape & Set Comparison Write-up

# Web Scrape

The English Short-Title Catalog contains more than 500,000 records and we wish to garner access, collect, and store all these that satisfy relevancy criteria to our region of interest, language of interest, and years of interest. To do as such we utilize web scraping techniques in Python, more specifically using packages of BeautifulSoup4, Selenium, Chrome Driver, and requests_html. In a more high level consideration, this code has two parts, (1) interacting with the parameter entry interface on ESTC and (2) clicking through and saving all the responses to each query we ask in an automated fashion. 

For (1) for each iteration we have several search parameters that act as constants; language code is ‘eng,’ country is ‘enk,’ and document type is ‘alldocuments.’ And we have some parameters that change with each iteration; each iteration includes one year in the range 1500–1800. After the algorithm enters the search parameters for the current year, iteration, the algorithm interacts with the ‘Go’ button on the page, Search button, waits for the page to refresh, and clicks on the hyperlinked number of results pop-ups, if there are non-zero documents with the requested criteria, and proceeds to go through (2). With the existence of some (2), the above code would be satisfactory to garner access to the entire corpus of texts with our desired parameters; however, ESTC and several other search platforms limit the number of search results that one can access with one search – the industry standard of 1001 (ESTC may report there are 3432 results for a given year, but you can only access the FIRST 1001). Meaning each inquiry is capped to produce at a max 1001 results to be scraped by (2) – which proves to be a problematic feature especially for the later years in British history. To circumvent this, we increase our number of iterations by shifting our economic unit of measure from the year to sub year intervals. We achieve this through the logical parameters that ESTC allows users to add to their search inquiries. We produce a multi-level depth-based logic decision tree that acts as follows. If the number of works for a given year exceeds 1001, then apply the first tree level of logic with AND, and repeat with NOT. If the first logic term, with AND or NOT, exceeds 1001, add an additional logic level and consider both AND and NOT. We incorporate 4 levels to this decision tree. For our first level we start with using where it was published; AND London or NOT London. To further partition the search results, where necessary, at lower levels we utilize some of the most common words in the English language such as be, an, I, in, on, by, and more. With (1) in its full form with these additional logic considerations repeating for multiple iterations each year we are able to access 99.8% of the works that satisfy our desired parameters (i.e. 1400->1800, eng, enk, alldocuments).

Access is nice, but without (2) we have no viable method of storing these search results bar years of manual labor. For each iteration, as described above, we need to feed its output, the hyperlink that produces the search results, to an algorithm that interacts with each search result on their own page, and saves the relevant information into a useful data structure, and appends each result to this data structure. We implement an algorithm that does just that with bs4, selenium and requests_html. Once equipped with this search results link the scraping code goes as follows; (a) clicks on the first search result, think an Amazon page where you click on the first item, (b) stores the metrics of interest of this 1 search result (title, publisher, author, year, meta-data, etc.), (c) finds the ‘Next button,’ (d) clicks on said ‘Next Button’ if it is live (blue not gray), and (e) repeats a-d until there is no live next result button. This a-e process occurs for each iteration, each year with its subsequent run’s logic. 

Note, since the search parameters do not produce perfect partitions, we ended up doing some additional scraping that was not necessary, i.e. rescraping a book multiple times. However, in that 99.8% of works scraped, we already removed duplicate scrapings from our data set. Meaning this 99.8% is not biased in any way and is a true 99.8% collection of the universe of documents in our parameter set as of October 2023. 


## Set Comparisons 

To compare the ESTC scraped data to the Hathitrust data requires an additional algorithm. Since our use case is to compare book titles, strings, to one another with varying degrees of conventional, modern, spelling over the evolution of the English language, the standard computer scientific notion of exact equivalence will not be satisfactory (i.e. ‘Apple’ and ‘apple’ are not the same since the hash of A != the hash of a). There exist packages that get at this exact use case. I choose to utilize the ‘fuzzywuzzy’ library that handles these ‘fuzzy’ or weak string matches we desire. In particular I utilize ‘rapidfuzz’ as its implementation is meant for larger data sets and produces greater efficiency in these cases. 

In both data sets, henceforth ESTC_df and hathitrust_df, they contain a column of Titles, IDs, Authors, etc. Namely, each row in each data set correspond to a different book Title. In string comparison, I perform standard pre-processing techniques, such as removing leading and lagging punctuation, removing capitalization, etc. With these conventions done, I leverage this organization feature to do as follows; take a row, book title, in ESTC_df and compare it against all the Titles in hathitrust_df in each comparison calculate the match score (metric from 0–100 of how similar the two strings are), then produce a ‘Best Match’ by searching for the max of these Real numbers, and create a new data frame corresponding to the Title in ESTC_df, it’s best match found in hathitrust_df, their respective ID’s in each, and the Match Score – this process works vice-versa. 

With this data frame constructed, one can readily create an intersection and set difference by partitioning the data based on some real number threshold value. For our case we utilize 75; a book is included in the intersection if their match is greater than or equal to the threshold value, otherwise they are in the set difference data frame. With this threshold and comparing hathitrust_df to ESTC_DF, we find the intersection to 18XXX which is XX.XX% of the data from this period (mention the limited year overlap). 

Note, these Best Matches are informative for finding books in the two data sets that might be related or the same, but it does produce errors. When two books are recognized as being the same, placed in the intersection data frame, there is potential for confusion on the fuzz’s internal algorithm (e.g. ‘The life of Edgar’ and ‘The life of Edmond’ would garner a high similarity score, but may in-fact be about completely different people). It is also the case, that books that do have a best match exceeding the threshold may be incorrectly placed into the set difference data frame; in fuzz’s algorithm, shorter strings are punished harder for any differences between the two whereas longer strings are more easily forgiven. This notion of match is looser and likely to be slightly biased, in both directions. 
![image](https://github.com/AaronSVMB/WebScrape/assets/124106475/69cf30fc-cb44-4280-bdf4-a827f5f4a0a9)
